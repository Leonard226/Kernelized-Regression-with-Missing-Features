{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2e81b29c71eb2b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Kernelized Regression with Missing Features\n",
    "In this task, we predict electricity prices in Switzerland using price information from other countries and additional features. Specifically, we are provided with electricity prices for certain seasons (spring, summer, autumn, winter) over the past few years in Switzerland and some other countries. Additionally, we need to address missing features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20bc99-7c40-4325-9ae9-ea6e24a98350",
   "metadata": {},
   "source": [
    "#### Imports & Reading Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e071b8e282a8d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T18:47:37.485752Z",
     "start_time": "2024-03-10T18:47:37.479263Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: \n",
      " [['spring' nan 9.644027877268496 ... -3.931031226630509 nan\n",
      "  -3.238196806151894]\n",
      " ['summer' nan 7.246060839801865 ... nan nan -3.212894038068976]\n",
      " ['autumn' -2.101936612395246 7.620084525124941 ... -4.07384968174626 nan\n",
      "  -3.1140608060213903]\n",
      " ...\n",
      " ['summer' -0.9711569246205298 nan ... -1.4993613445447886\n",
      "  3.110638067512592 2.230252561735496]\n",
      " ['autumn' nan nan ... -1.5477160129737388 3.105416529245648\n",
      "  1.989139721317721]\n",
      " ['winter' -1.0583425835760634 nan ... nan 3.272815718725681\n",
      "  2.080666809994271]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RBF, Matern, RationalQuadratic\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "data_train = train_df.to_numpy()\n",
    "data_test = test_df.to_numpy() # note that data_test does not contain price_CHF column\n",
    "print('Training Data: \\n', data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f2086e18dd7b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Data Preprocessing\n",
    "Typical imputation strategies to deal with missing data include: \n",
    "- discarding rows or columns with missing data\n",
    "- replacing missing values with the corresponding mean or median\n",
    "- advanced imputation strategies based on iterative model fitting\n",
    "\n",
    "References (data imputation): \n",
    "- kaggle notebook: https://www.kaggle.com/code/residentmario/simple-techniques-for-missing-data-imputation/notebook\n",
    "- sklearn website: https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "References (handling categorical data):\n",
    "- kaggle notebook: https://www.kaggle.com/code/alexisbcook/categorical-variables\n",
    "- sklearn website: https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\n",
    "\n",
    "\n",
    "\n",
    "#### Categorical Encoding\n",
    "We use ordinal encoding to deal with the categorical variables, i.e. we use the following mapping: spring $\\mapsto 1.0$, summer $\\mapsto 2.0$, autumn $\\mapsto 3.0$, winter $\\mapsto 4.0$. Scikit-learn has a `OrdinalEncoder` class that can be used to obtain the ordinal encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "402e111cb0d70236",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: \n",
      " [[1.0 nan 9.644027877268496 ... -3.931031226630509 nan -3.238196806151894]\n",
      " [2.0 nan 7.246060839801865 ... nan nan -3.212894038068976]\n",
      " [0.0 -2.101936612395246 7.620084525124941 ... -4.07384968174626 nan\n",
      "  -3.1140608060213903]\n",
      " ...\n",
      " [2.0 -0.9711569246205298 nan ... -1.4993613445447886 3.110638067512592\n",
      "  2.230252561735496]\n",
      " [0.0 nan nan ... -1.5477160129737388 3.105416529245648 1.989139721317721]\n",
      " [3.0 -1.0583425835760634 nan ... nan 3.272815718725681 2.080666809994271]]\n"
     ]
    }
   ],
   "source": [
    "# categorical encoding\n",
    "categorical_column_train = data_train[:, 0].reshape(-1, 1)  \n",
    "numerical_columns_train = data_train[:, 1:]  \n",
    "categorical_column_test = data_test[:, 0].reshape(-1, 1)\n",
    "numerical_columns_test = data_test[:, 1:]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoded_categorical_train = encoder.fit_transform(categorical_column_train)\n",
    "encoded_categorical_test = encoder.fit_transform(categorical_column_test)\n",
    "\n",
    "# Concatenate the encoded categorical column with the numerical columns\n",
    "data_train = np.hstack((encoded_categorical_train, numerical_columns_train))\n",
    "data_test = np.hstack((encoded_categorical_test, numerical_columns_test))\n",
    "print('Training Data: \\n', data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4458c15-cc7a-4a41-bcc4-7d2872cd832d",
   "metadata": {},
   "source": [
    "#### Data Imputation\n",
    "We use `IterativeImputer` from Scikit-kearn to impute missing values. It fills in missing values iteratively by modeling each feature with missing values as a function of the other features. This is done using a regression model that predicts the missing values based on the available data. The imputation process involves mulitple iterations. In each iteration, the algorithm estimates the missing values, update the dataset with these estimates, and then refines the estimates in subsequent iterations. This is done until convergence or the maximum number of iterations is reached. \n",
    "\n",
    "By modeling each feature as a function of others, `IterativeImputer` can capture complex relationships between features and provide more accurate imputations compared to methods like mean imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "625768ad-ff6c-4a76-a137-9df8a691aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: \n",
      " [[ 1.         -1.94730966  9.64402788 ... -3.93103123 -2.6863533\n",
      "  -3.23819681]\n",
      " [ 2.         -2.17086395  7.24606084 ... -3.99871774 -2.4877255\n",
      "  -3.21289404]\n",
      " [ 0.         -2.10193661  7.62008453 ... -4.07384968 -2.43497284\n",
      "  -3.11406081]\n",
      " ...\n",
      " [ 2.         -0.97115692 -0.44059049 ... -1.49936134  3.11063807\n",
      "   2.23025256]\n",
      " [ 0.         -1.14346076 -0.33911152 ... -1.54771601  3.10541653\n",
      "   1.98913972]\n",
      " [ 3.         -1.05834258 -0.88944359 ... -1.26471153  3.27281572\n",
      "   2.08066681]]\n"
     ]
    }
   ],
   "source": [
    "# data imputation\n",
    "imp = IterativeImputer(max_iter=100, random_state=0)\n",
    "data_train = imp.fit_transform(data_train)\n",
    "data_test = imp.fit_transform(data_test)\n",
    "print('Training Data: \\n', data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959037466887e870",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Modeling and Prediction\n",
    "We aim to use a kernelized estimator, with the challenge being to select the appropriate kernel for the regression. Commonly used kernels include linear, squared exponential (RBF), polynomial, Matern, and RationalQuadratic kernels. To determine the optimal kernel, we apply the $K$-fold cross-validation technique. After identifying the optimal kernel, we train our model on the entire training set and make final predictions on the test set.\n",
    "\n",
    "We use the `GaussianProcessRegressor` framework for training and making predictions.\n",
    "\n",
    "Note that we use the RÂ²-score metric to assess the quality of our model, which is defined as:\n",
    "$$R^2(y^*, y) = 1 - \\frac{\\sum_{i=1}^N (y_i - y_i^*)^2}{\\sum_{i=1}^N(y_i - \\bar y)^2},$$\n",
    "where $\\bar y$ denotes the average of the true values $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fb0d86b605f9813",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R2-score: [1.         0.81978014 0.85539344 0.85409033]\n",
      "Optimal Kernel: DotProduct(sigma_0=1)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.delete(data_train, 2, axis=1)\n",
    "y_train = data_train[:, 2]\n",
    "X_test = data_test\n",
    "\n",
    "K = 10  # number of folds\n",
    "kernels = [DotProduct(), RBF(), Matern(), RationalQuadratic()]\n",
    "Error_mat = np.zeros((K, len(kernels)))\n",
    "kf = KFold(n_splits=K)\n",
    "\n",
    "# Cross-validation loop\n",
    "for i, (train, test) in enumerate(kf.split(X_train)):\n",
    "    for j, kernel in enumerate(kernels): \n",
    "        # training model \n",
    "        X_train_cv, y_train_cv = X_train[train], y_train[train]\n",
    "        gpr = GaussianProcessRegressor(kernel=kernel)\n",
    "        gpr.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        # testing model \n",
    "        X_test_cv, y_test_cv = X_train[test], y_train[test]\n",
    "        y_pred_cv = gpr.predict(X_test_cv)\n",
    "        Error_mat[i,j] = r2_score(y_test_cv, y_pred_cv)\n",
    "\n",
    "\n",
    "# Averaging R2 scores across folds for each kernel\n",
    "avg_r2_score = np.mean(Error_mat, axis=0)\n",
    "print('Average R2-score:', avg_r2_score)\n",
    "\n",
    "# figuring out the optimal kernel\n",
    "max = np.argmax(avg_r2_score)\n",
    "kernel_opt = kernels[max]\n",
    "print('Optimal Kernel:', kernel_opt)\n",
    "\n",
    "# training final model with optimal kernel on full training set\n",
    "gpr = GaussianProcessRegressor(kernel=kernel_opt)\n",
    "gpr.fit(X_train, y_train)\n",
    "# making prediction on test set\n",
    "y_pred = gpr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c62e0cd4cec5a7e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "382d87d2d67ddbdc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results file successfully generated!\n"
     ]
    }
   ],
   "source": [
    "dt = pd.DataFrame(y_pred) \n",
    "dt.columns = ['price_CHF']\n",
    "dt.to_csv('results.csv', index=False)\n",
    "print(\"\\nResults file successfully generated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
